

===============================================================================
===============================================================================

APPENDIX 3 - Storage

===============================================================================
===============================================================================


===========
1 - Volumes
===========


On-disk files in a Container are ephemeral, which presents some problems for 
non-trivial applications when running in Containers. First, when a Container 
crashes, kubelet will restart it, but the files will be lost - the Container 
starts with a clean state. Second, when running Containers together in a Pod it 
is often necessary to share files between those Containers. The Kubernetes 
Volume abstraction solves both of these problems.



1.1 - Background
================


Docker also has a concept of volumes, though it is somewhat looser and less 
managed. In Docker, a volume is simply a directory on disk or in another 
Container. Lifetimes are not managed and until very recently there were only 
local-disk-backed volumes. Docker now provides volume drivers, but the 
functionality is very limited for now (e.g. as of Docker 1.7 only one volume 
driver is allowed per Container and there is no way to pass parameters to 
volumes).

A Kubernetes volume, on the other hand, has an explicit lifetime - the same as 
the Pod that encloses it. Consequently, a volume outlives any Containers that 
run within the Pod, and data is preserved across Container restarts. Of course, 
when a Pod ceases to exist, the volume will cease to exist, too. Perhaps more 
importantly than this, Kubernetes supports many types of volumes, and a Pod can 
use any number of them simultaneously.

At its core, a volume is just a directory, possibly with some data in it, which 
is accessible to the Containers in a Pod. How that directory comes to be, the 
medium that backs it, and the contents of it are determined by the particular 
volume type used.

To use a volume, a Pod specifies what volumes to provide for the Pod (the 
.spec.volumes field) and where to mount those into Containers (the 
.spec.containers[*].volumeMounts field).

A process in a container sees a filesystem view composed from their Docker 
image and volumes. The Docker image is at the root of the filesystem hierarchy, 
and any volumes are mounted at the specified paths within the image. Volumes 
cannot mount onto other volumes or have hard links to other volumes. Each 
Container in the Pod must independently specify where to mount each volume.


1.2 - Types of Volumes
======================


Kubernetes supports many types of Volumes: local, hostPath, emptyDir, cephfs, 
glusterfs, csi, cinder, nfs, persistentVolumeClaim, awsElasticBlockStore, 
azureDisk, azureFile, gcePersistentDisk, vsphereVolume, configMap, downwardAPI, 
fc (fibre channel), flexVolume, flocker, gitRepo (deprecated), iscsi, 
projected, portworxVolume, quobyte, rbd, scaleIO, secret, storageos...

    
We will ignore here most options (namely all the major cloud provider's 
solutions and focus on very few which can be deployed under the control of the 
local team. The choice is de facto subjective, but the range of possible option 
is so wide that we HAVE to make a choice.


1.2.1 - hostPath
================

A hostPath volume mounts a file or directory from the host node’s filesystem 
into your Pod. This is not something that most Pods will need, but it offers a 
powerful escape hatch for some applications.

For example, some uses for a hostPath are:

    - running a Container that needs access to Docker internals; use a hostPath 
        of /var/lib/docker
    - running cAdvisor in a Container; use a hostPath of /sys
    - allowing a Pod to specify whether a given hostPath should exist prior to 
        the Pod running, whether it should be created, and what it should exist 
        as
    - demonstrating non-storage related applications without having to deploy 
        sophisticated solutions like glusterfs or similar

In addition to the required path property, user can optionally specify a type 
for a hostPath volume.

The supported values for field type are:

    Value               Behavior
                        Empty string (default) is for backward compatibility, 
                        which means that no checks will be performed before 
                        mounting the hostPath volume.
    DirectoryOrCreate   If nothing exists at the given path, an empty directory 
                        will be created there as needed with permission set to 
                        0755, having the same group and ownership with Kubelet.
    Directory           A directory must exist at the given path
    FileOrCreate        If nothing exists at the given path, an empty file will 
                        be created there as needed with permission set to 0644, 
                        having the same group and ownership with Kubelet.
    File                A file must exist at the given path
    Socket              A UNIX socket must exist at the given path
    CharDevice          A character device must exist at the given path
    BlockDevice         A block device must exist at the given path

Watch out when using this type of volume, because:

    - Pods with identical configuration (such as created from a podTemplate) 
        may behave differently on different nodes due to different files on the 
        nodes
    - when Kubernetes adds resource-aware scheduling, as is planned, it will 
        not be able to account for resources used by a hostPath
    - the files or directories created on the underlying hosts are only 
        writable by root. You either need to run your process as root in a 
        privileged Container or modify the file permissions on the host to be 
        able to write to a hostPath volume

Example Pod (YAML file)

    apiVersion: v1
    kind: Pod
    metadata:
    name: test-pd
    spec:
    containers:
    - image: k8s.gcr.io/test-webserver
        name: test-container
        volumeMounts:
        - mountPath: /test-pd
        name: test-volume
    volumes:
    - name: test-volume
        hostPath:
        # directory location on host
        path: /data
        # this field is optional
        type: Directory


1.2.2 - local
=============


A local volume represents a mounted local storage device such as a disk, 
partition or directory.

Local volumes can only be used as a statically created PersistentVolume. 
Dynamic provisioning is not supported yet.

Compared to hostPath volumes, local volumes can be used in a durable and 
portable manner without manually scheduling Pods to nodes, as the system is 
aware of the volume’s node constraints by looking at the node affinity on the 
PersistentVolume.

However, local volumes are still subject to the availability of the underlying 
node and are not suitable for all applications. If a node becomes unhealthy, 
then the local volume will also become inaccessible, and a Pod using it will 
not be able to run. Applications using local volumes must be able to tolerate 
this reduced availability, as well as potential data loss, depending on the 
durability characteristics of the underlying disk.

The following is an example of PersistentVolume spec using a local volume and 
nodeAffinity:

    apiVersion: v1
    kind: PersistentVolume
    metadata:
    name: example-pv
    spec:
    capacity:
        storage: 100Gi
    volumeMode: Filesystem
    accessModes:
    - ReadWriteOnce
    persistentVolumeReclaimPolicy: Delete
    storageClassName: local-storage
    local:
        path: /mnt/disks/ssd1
    nodeAffinity:
        required:
        nodeSelectorTerms:
        - matchExpressions:
            - key: kubernetes.io/hostname
            operator: In
            values:
            - example-node

PersistentVolume nodeAffinity is required when using local volumes. It enables 
the Kubernetes scheduler to correctly schedule Pods using local volumes to the 
correct node.

PersistentVolume volumeMode can be set to “Block” (instead of the default value 
“Filesystem”) to expose the local volume as a raw block device.

When using local volumes, it is recommended to create a StorageClass with 
volumeBindingMode set to WaitForFirstConsumer. See the example. Delaying volume 
binding ensures that the PersistentVolumeClaim binding decision will also be 
evaluated with any other node constraints the Pod may have, such as node 
resource requirements, node selectors, Pod affinity, and Pod anti-affinity.

An external static provisioner can be run separately for improved management of 
the local volume lifecycle. Note that this provisioner does not support dynamic 
provisioning yet. For an example on how to run an external local provisioner, 
see the local volume provisioner user guide 
(https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner).

    Note: The local PersistentVolume requires manual cleanup and deletion by 
        the user if the external static provisioner is not used to manage the 
        volume lifecycle.


1.2.3 - emptyDir
================

An emptyDir volume is first created when a Pod is assigned to a Node, and 
exists as long as that Pod is running on that node. As the name says, it is 
initially empty. Containers in the Pod can all read and write the same files in 
the emptyDir volume, though that volume can be mounted at the same or different 
paths in each Container. When a Pod is removed from a node for any reason, the 
data in the emptyDir is deleted forever.

    Note: A Container crashing does NOT remove a Pod from a node, so the data 
        in an emptyDir volume is safe across Container crashes.

Some uses for an emptyDir are:

    - scratch space, such as for a disk-based merge sort
    - checkpointing a long computation for recovery from crashes
    - holding files that a content-manager Container fetches while a webserver 
        Container serves the data

By default, emptyDir volumes are stored on whatever medium is backing the node 
- that might be disk or SSD or network storage, depending on your environment. 
However, you can set the emptyDir.medium field to "Memory" to tell Kubernetes 
to mount a tmpfs (RAM-backed filesystem) for you instead. While tmpfs is very 
fast, be aware that unlike disks, tmpfs is cleared on node reboot and any files 
you write will count against your Container’s memory limit.

Example Pod (YAML file)

    apiVersion: v1
    kind: Pod
    metadata:
    name: test-pd
    spec:
    containers:
    - image: k8s.gcr.io/test-webserver
        name: test-container
        volumeMounts:
        - mountPath: /cache
        name: cache-volume
    volumes:
    - name: cache-volume
        emptyDir: {}


1.2.x - cephfs
==============

A cephfs volume allows an existing CephFS volume to be mounted into your Pod. 
Unlike emptyDir, which is erased when a Pod is removed, the contents of a 
cephfs volume are preserved and the volume is merely unmounted. This means that 
a CephFS volume can be pre-populated with data, and that data can be “handed 
off” between Pods. CephFS can be mounted by multiple writers simultaneously.

    Caution: You must have your own Ceph server running with the share exported 
        before you can use it.

See the CephFS example for more details:

    - Set up a containrized Ceph Cluster: 
        https://github.com/ceph/ceph-container/tree/master/examples/kubernetes 
        This Guide will take you through the process of deploying a Ceph 
        cluster on to a Kubernetes cluster.
    - How to use it: 
        https://github.com/kubernetes/examples/tree/master/volumes/cephfs/

    
1.2.x - glusterfs
=================

A glusterfs volume allows a Glusterfs (an open source networked filesystem) 
volume to be mounted into your Pod. Unlike emptyDir, which is erased when a Pod 
is removed, the contents of a glusterfs volume are preserved and the volume is 
merely unmounted. This means that a glusterfs volume can be pre-populated with 
data, and that data can be “handed off” between Pods. GlusterFS can be mounted 
by multiple writers simultaneously.

    Caution: You must have your own GlusterFS installation running before you 
        can use it.

See the GlusterFS example for more details:
    - setting up your own gluterfs server: https://docs.gluster.org/en/latest/
    - installing glusterfs agents on the Kubernetes nodes: 
        https://gluster.readthedocs.io/en/latest/Administrator%20Guide/
    - using GlusterFS with Kubernetes: 
        https://github.com/kubernetes/examples/tree/master/volumes/glusterfs




1.2.4 - nfs
===========

An nfs volume allows an existing NFS (Network File System) share to be mounted 
into your Pod. Unlike emptyDir, which is erased when a Pod is removed, the 
contents of an nfs volume are preserved and the volume is merely unmounted. 
This means that an NFS volume can be pre-populated with data, and that data can 
be “handed off” between Pods. NFS can be mounted by multiple writers 
simultaneously.

    Caution: You must have your own NFS server running with the share exported 
        before you can use it.

See the NFS example for more details: 
https://github.com/kubernetes/examples/tree/master/staging/volumes/nfs


1.2.5 - scaleIO
====================


ScaleIO is a software-based storage platform that can use existing hardware to 
create clusters of scalable shared block networked storage. The scaleIO volume 
plugin allows deployed Pods to access existing ScaleIO volumes (or it can 
dynamically provision new volumes for persistent volume claims, see ScaleIO 
Persistent Volumes).

    Caution: You must have an existing ScaleIO cluster already setup and 
    running with the volumes created before you can use them.

ScaleIO has a native integration with Kubernetes: see and example here:
https://github.com/thecodeteam/labs/tree/master/setup-scaleio-vagrant

The following is an example of Pod configuration with ScaleIO:

    apiVersion: v1
    kind: Pod
    metadata:
    name: pod-0
    spec:
    containers:
    - image: k8s.gcr.io/test-webserver
        name: pod-0
        volumeMounts:
        - mountPath: /test-pd
        name: vol-0
    volumes:
    - name: vol-0
        scaleIO:
        gateway: https://localhost:443/api
        system: scaleio
        protectionDomain: sd0
        storagePool: sp1
        volumeName: vol-0
        secretRef:
            name: sio-secret
        fsType: xfs

For further detail, please see the ScaleIO examples.
https://github.com/kubernetes/examples/tree/master/staging/volumes/scaleio


1.2.6 - storageOS
======================

A storageos volume allows an existing StorageOS volume to be mounted into your 
Pod.

StorageOS runs as a Container within your Kubernetes environment, making local 
or attached storage accessible from any node within the Kubernetes cluster. 
Data can be replicated to protect against node failure. Thin provisioning and 
compression can improve utilization and reduce cost.

At its core, StorageOS provides block storage to Containers, accessible via a 
file system.

The StorageOS Container requires 64-bit Linux and has no additional 
dependencies. A free developer license is available.
https://play.storageos.com/k8s-3nodes

    Caution: You must run the StorageOS Container on each node that wants to 
        access StorageOS volumes or that will contribute storage capacity to 
        the pool. For installation instructions, consult the StorageOS 
        documentation.

    apiVersion: v1
    kind: Pod
    metadata:
    labels:
        name: redis
        role: master
    name: test-storageos-redis
    spec:
    containers:
        - name: master
        image: kubernetes/redis:v1
        env:
            - name: MASTER
            value: "true"
        ports:
            - containerPort: 6379
        volumeMounts:
            - mountPath: /redis-master-data
            name: redis-data
    volumes:
        - name: redis-data
        storageos:
            # The `redis-vol01` volume must already exist within StorageOS in the `default` namespace.
            volumeName: redis-vol01
            fsType: ext4

For more information including Dynamic Provisioning and Persistent Volume 
Claims, please see the StorageOS examples.
https://github.com/kubernetes/examples/tree/master/volumes/storageos


1.2.7 - portworxVolume
=========================


A portworxVolume is an elastic block storage layer that runs hyperconverged 
with Kubernetes. Portworx fingerprints storage in a server, tiers based on 
capabilities, and aggregates capacity across multiple servers. Portworx runs 
in-guest in virtual machines or on bare metal Linux nodes.

A portworxVolume can be dynamically created through Kubernetes or it can also 
be pre-provisioned and referenced inside a Kubernetes Pod. Here is an example 
Pod referencing a pre-provisioned PortworxVolume:

    apiVersion: v1
    kind: Pod
    metadata:
    name: test-portworx-volume-pod
    spec:
    containers:
    - image: k8s.gcr.io/test-webserver
        name: test-container
        volumeMounts:
        - mountPath: /mnt
        name: pxvol
    volumes:
    - name: pxvol
        # This Portworx volume must already exist.
        portworxVolume:
        volumeID: "pxvol"
        fsType: "<fs-type>"

    Caution: Make sure you have an existing PortworxVolume with name pxvol 
        before using it in the Pod.

More details and examples can be found here:
https://portworx.com/use-case/kubernetes-storage/

BEWARE: portworx is not open source!!!



1.3 - Using subPath
===================


Sometimes, it is useful to share one volume for multiple uses in a single Pod. 
The volumeMounts.subPath property can be used to specify a sub-path inside the 
referenced volume instead of its root.

Here is an example of a Pod with a LAMP stack (Linux Apache Mysql PHP) using a 
single, shared volume. The HTML contents are mapped to its html folder, and the 
databases will be stored in its mysql folder:

    apiVersion: v1
    kind: Pod
    metadata:
    name: my-lamp-site
    spec:
        containers:
        - name: mysql
        image: mysql
        env:
        - name: MYSQL_ROOT_PASSWORD
            value: "rootpasswd"
        volumeMounts:
        - mountPath: /var/lib/mysql
            name: site-data
            subPath: mysql
        - name: php
        image: php:7.0-apache
        volumeMounts:
        - mountPath: /var/www/html
            name: site-data
            subPath: html
        volumes:
        - name: site-data
        persistentVolumeClaim:
            claimName: my-lamp-site-data

            
1.4 - Resources
===============


The storage media (Disk, SSD, etc.) of an emptyDir volume is determined by the 
medium of the filesystem holding the kubelet root dir (typically 
/var/lib/kubelet). There is no limit on how much space an emptyDir or hostPath 
volume can consume, and no isolation between Containers or between Pods.

In the future, we expect that emptyDir and hostPath volumes will be able to 
request a certain amount of space using a resource specification, and to select 
the type of media to use, for clusters that have several media types.


1.5 - Out-of-Tree Volume Plugins
================================


The Out-of-tree volume plugins include the Container Storage Interface (CSI) 
and FlexVolume. They enable storage vendors to create custom storage plugins 
without adding them to the Kubernetes repository.

Before the introduction of CSI and FlexVolume, all volume plugins (like volume 
types listed above) were “in-tree” meaning they were built, linked, compiled, 
and shipped with the core Kubernetes binaries and extend the core Kubernetes 
API. This meant that adding a new storage system to Kubernetes (a volume 
plugin) required checking code into the core Kubernetes code repository.

Both CSI and FlexVolume allow volume plugins to be developed independent of the 
Kubernetes code base, and deployed (installed) on Kubernetes clusters as 
extensions.


1.5.1 - CSI
===========

Container Storage Interface (CSI) defines a standard interface for container 
orchestration systems (like Kubernetes) to expose arbitrary storage systems to 
their container workloads.

Once a CSI compatible volume driver is deployed on a Kubernetes cluster, users 
may use the csi volume type to attach, mount, etc. the volumes exposed by the 
CSI driver.

The csi volume type does not support direct reference from Pod and may only be 
referenced in a Pod via a PersistentVolumeClaim object.


1.5.2 - FlexVolume
==================

FlexVolume is an out-of-tree plugin interface that has existed in Kubernetes 
since version 1.2 (before CSI). It uses an exec-based model to interface with 
drivers. FlexVolume driver binaries must be installed in a pre-defined volume 
plugin path on each node (and in some cases master).

Pods interact with FlexVolume drivers through the flexvolume in-tree plugin. 
More details can be found here:
https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md



1.6 - Mount propagation
=======================


Mount propagation allows for sharing volumes mounted by a Container to other 
Containers in the same Pod, or even to other Pods on the same node.

Mount propagation of a volume is controlled by mountPropagation field in 
Container.volumeMounts. Its values are:

    - None - This volume mount will not receive any subsequent mounts that are 
        mounted to this volume or any of its subdirectories by the host. In 
        similar fashion, no mounts created by the Container will be visible on 
        the host. This is the default mode.
        This mode is equal to private mount propagation as described in the 
        Linux kernel documentation

    - HostToContainer - This volume mount will receive all subsequent mounts 
        that are mounted to this volume or any of its subdirectories.
        In other words, if the host mounts anything inside the volume mount, 
        the Container will see it mounted there.
        Similarly, if any Pod with Bidirectional mount propagation to the same 
        volume mounts anything there, the Container with HostToContainer mount 
        propagation will see it.
        This mode is equal to rslave mount propagation as described in the 
        Linux kernel documentation

    - Bidirectional - This volume mount behaves the same the HostToContainer 
        mount. In addition, all volume mounts created by the Container will be 
        propagated back to the host and to all Containers of all Pods that use 
        the same volume.
        A typical use case for this mode is a Pod with a FlexVolume or CSI 
        driver or a Pod that needs to mount something on the host using a 
        hostPath volume.
        This mode is equal to rshared mount propagation as described in the 
        Linux kernel documentation

    Caution: Bidirectional mount propagation can be dangerous. It can damage 
        the host operating system and therefore it is allowed only in 
        privileged Containers. Familiarity with Linux kernel behavior is 
        strongly recommended. In addition, any volume mounts created by 
        Containers in Pods must be destroyed (unmounted) by the Containers on 
        termination.

Configuration
=============

Before mount propagation can work properly on some deployments (CoreOS, 
RedHat/Centos, Ubuntu) mount share must be configured correctly in Docker as 
shown below.

Edit your Docker’s systemd service file. Set MountFlags as follows:

    MountFlags=shared

    Or, remove MountFlags=slave if present.

Then restart the Docker daemon:

    sudo systemctl daemon-reload
    sudo systemctl restart docker




======================
2 - Persistent Volumes
======================

This document describes the current state of PersistentVolumes in Kubernetes. 
Familiarity with volumes is suggested.


2.1 - Introduction
==================


Managing storage is a distinct problem from managing compute instances. The 
PersistentVolume subsystem provides an API for users and administrators that 
abstracts details of how storage is provided from how it is consumed. To do 
this, we introduce two new API resources: PersistentVolume and 
PersistentVolumeClaim.

A PersistentVolume (PV) is a piece of storage in the cluster that has been 
provisioned by an administrator or dynamically provisioned using Storage 
Classes. It is a resource in the cluster just like a node is a cluster 
resource. PVs are volume plugins like Volumes, but have a lifecycle independent 
of any individual Pod that uses the PV. This API object captures the details of 
the implementation of the storage, be that NFS, iSCSI, or a 
cloud-provider-specific storage system.

A PersistentVolumeClaim (PVC) is a request for storage by a user. It is similar 
to a Pod. Pods consume node resources and PVCs consume PV resources. Pods can 
request specific levels of resources (CPU and Memory). Claims can request 
specific size and access modes (e.g., they can be mounted once read/write or 
many times read-only).

While PersistentVolumeClaims allow a user to consume abstract storage 
resources, it is common that users need PersistentVolumes with varying 
properties, such as performance, for different problems. Cluster administrators 
need to be able to offer a variety of PersistentVolumes that differ in more 
ways than just size and access modes, without exposing users to the details of 
how those volumes are implemented. For these needs, there is the StorageClass 
resource.

See the detailed walkthrough with working examples.
https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/


2.2 - Lifecycle of a volume and claim
=====================================


PVs are resources in the cluster. PVCs are requests for those resources and 
also act as claim checks to the resource. The interaction between PVs and PVCs 
follows this lifecycle:


2.2.1 - Provisioning
====================

There are two ways PVs may be provisioned: statically or dynamically.

Static
======
A cluster administrator creates a number of PVs. They carry the details of the 
real storage, which is available for use by cluster users. They exist in the 
Kubernetes API and are available for consumption.

Dynamic
=======

When none of the static PVs the administrator created match a user’s 
PersistentVolumeClaim, the cluster may try to dynamically provision a volume 
specially for the PVC. This provisioning is based on StorageClasses: the PVC 
must request a storage class and the administrator must have created and 
configured that class for dynamic provisioning to occur. Claims that request 
the class "" effectively disable dynamic provisioning for themselves.

To enable dynamic storage provisioning based on storage class, the cluster 
administrator needs to enable the DefaultStorageClass admission controller on 
the API server. This can be done, for example, by ensuring that 
DefaultStorageClass is among the comma-delimited, ordered list of values for 
the --enable-admission-plugins flag of the API server component. For more 
information on API server command-line flags, check kube-apiserver 
documentation.


2.2.2 - Binding
===============

A user creates, or in the case of dynamic provisioning, has already created, a 
PersistentVolumeClaim with a specific amount of storage requested and with 
certain access modes. A control loop in the master watches for new PVCs, finds 
a matching PV (if possible), and binds them together. If a PV was dynamically 
provisioned for a new PVC, the loop will always bind that PV to the PVC. 
Otherwise, the user will always get at least what they asked for, but the 
volume may be in excess of what was requested. Once bound, 
PersistentVolumeClaim binds are exclusive, regardless of how they were bound. A 
PVC to PV binding is a one-to-one mapping.

Claims will remain unbound indefinitely if a matching volume does not exist. 
Claims will be bound as matching volumes become available. For example, a 
cluster provisioned with many 50Gi PVs would not match a PVC requesting 100Gi. 
The PVC can be bound when a 100Gi PV is added to the cluster.


2.2.3 - Using
=============

Pods use claims as volumes. The cluster inspects the claim to find the bound 
volume and mounts that volume for a Pod. For volumes that support multiple 
access modes, the user specifies which mode is desired when using their claim 
as a volume in a Pod.

Once a user has a claim and that claim is bound, the bound PV belongs to the 
user for as long as they need it. Users schedule Pods and access their claimed 
PVs by including a persistentVolumeClaim in their Pod’s volumes block. See 
below for syntax details.


2.2.4 - Storage Object in Use Protection
========================================

The purpose of the Storage Object in Use Protection feature is to ensure that 
Persistent Volume Claims (PVCs) in active use by a Pod and Persistent Volume 
(PVs) that are bound to PVCs are not removed from the system, as this may 
result in data loss.

    Note: PVC is in active use by a Pod when a Pod object exists that is using 
        the PVC.

If a user deletes a PVC in active use by a Pod, the PVC is not removed 
immediately. PVC removal is postponed until the PVC is no longer actively used 
by any Pods. Also, if an admin deletes a PV that is bound to a PVC, the PV is 
not removed immediately. PV removal is postponed until the PV is no longer 
bound to a PVC.

You can see that a PVC is protected when the PVC’s status is Terminating and 
the Finalizers list includes kubernetes.io/pvc-protection:

    kubectl describe pvc hostpath
    Name:          hostpath
    Namespace:     default
    StorageClass:  example-hostpath
    Status:        Terminating
    Volume:
    Labels:        <none>
    Annotations:   volume.beta.kubernetes.io/storage-class=example-hostpath
                volume.beta.kubernetes.io/storage-provisioner=example.com/hostpath
    Finalizers:    [kubernetes.io/pvc-protection]
    ...

You can see that a PV is protected when the PV’s status is Terminating and the 
Finalizers list includes kubernetes.io/pv-protection too:

    kubectl describe pv task-pv-volume
    Name:            task-pv-volume
    Labels:          type=local
    Annotations:     <none>
    Finalizers:      [kubernetes.io/pv-protection]
    StorageClass:    standard
    Status:          Available
    Claim:
    Reclaim Policy:  Delete
    Access Modes:    RWO
    Capacity:        1Gi
    Message:
    Source:
        Type:          HostPath (bare host directory volume)
        Path:          /tmp/data
        HostPathType:
    Events:            <none>


2.2.5 - Reclaiming
==================

When a user is done with their volume, they can delete the PVC objects from the 
API that allows reclamation of the resource. The reclaim policy for a 
PersistentVolume tells the cluster what to do with the volume after it has been 
released of its claim. Currently, volumes can either be Retained, Recycled, or 
Deleted.

Retain
======

The Retain reclaim policy allows for manual reclamation of the resource. When 
the PersistentVolumeClaim is deleted, the PersistentVolume still exists and the 
volume is considered “released”. But it is not yet available for another claim 
because the previous claimant’s data remains on the volume. An administrator 
can manually reclaim the volume with the following steps.

    1. Delete the PersistentVolume. The associated storage asset in external 
        infrastructure (such as an AWS EBS, GCE PD, Azure Disk, or Cinder 
        volume) still exists after the PV is deleted.
    2. Manually clean up the data on the associated storage asset accordingly.
    3. Manually delete the associated storage asset, or if you want to reuse 
        the same storage asset, create a new PersistentVolume with the storage 
        asset definition.

Delete
======

For volume plugins that support the Delete reclaim policy, deletion removes 
both the PersistentVolume object from Kubernetes, as well as the associated 
storage asset in the external infrastructure, such as an AWS EBS, GCE PD, Azure 
Disk, or Cinder volume. Volumes that were dynamically provisioned inherit the 
reclaim policy of their StorageClass, which defaults to Delete. The 
administrator should configure the StorageClass according to users’ 
expectations; otherwise, the PV must be edited or patched after it is created. 

Recycle
=======

    Warning: The Recycle reclaim policy is deprecated. Instead, the recommended 
        approach is to use dynamic provisioning.


2.2.6 - Expanding Persistent Volumes Claims
===========================================

Support for expanding PersistentVolumeClaims (PVCs) is now enabled by default. 
You can expand the following types of volumes:

    gcePersistentDisk
    awsElasticBlockStore
    Cinder
    glusterfs
    rbd
    Azure File
    Azure Disk
    Portworx
    FlexVolumes
    CSI

You can only expand a PVC if its storage class’s allowVolumeExpansion field is 
set to true.

    apiVersion: storage.k8s.io/v1
    kind: StorageClass
    metadata:
      name: gluster-vol-default
    provisioner: kubernetes.io/glusterfs
    parameters:
      resturl: "http://192.168.10.100:8080"
      restuser: ""
      secretNamespace: ""
      secretName: ""
    allowVolumeExpansion: true

To request a larger volume for a PVC, edit the PVC object and specify a larger 
size. This triggers expansion of the volume that backs the underlying 
PersistentVolume. A new PersistentVolume is never created to satisfy the claim. 
Instead, an existing volume is resized.

CSI Volume expansion
====================

Support for expanding CSI volumes is enabled by default but it also requires a 
specific CSI driver to support volume expansion. Refer to documentation of the 
specific CSI driver for more information.

Resizing a volume containing a file system
==========================================

You can only resize volumes containing a file system if the file system is XFS, 
Ext3, or Ext4.

When a volume contains a file system, the file system is only resized when a 
new Pod is using the PersistentVolumeClaim in ReadWrite mode. File system 
expansion is either done when a Pod is starting up or when a Pod is running and 
the underlying file system supports online expansion.

FlexVolumes allow resize if the driver is set with the RequiresFSResize 
capability to true. The FlexVolume can be resized on Pod restart.

Resizing an in-use PersistentVolumeClaim
========================================

In this case, you don’t need to delete and recreate a Pod or deployment that is 
using an existing PVC. Any in-use PVC automatically becomes available to its 
Pod as soon as its file system has been expanded. This feature has no effect on 
PVCs that are not in use by a Pod or deployment. You must create a Pod that 
uses the PVC before the expansion can complete.

Similar to other volume types - FlexVolume volumes can also be expanded when 
in-use by a Pod.


2.3 - Types of Persistent Volumes
=================================


PersistentVolume types are implemented as plugins. Kubernetes currently 
supports many plugins, amongst which the following may be used for the 
tutorial:

    NFS
    CephFS
    Glusterfs
    ScaleIO Volumes
    StorageOS


2.4 - Persistent Volumes
========================


Each PV contains a spec and status, which is the specification and status of 
the volume.

    apiVersion: v1
    kind: PersistentVolume
    metadata:
      name: pv0003
    spec:
      capacity:
        storage: 5Gi
      volumeMode: Filesystem
      accessModes:
        - ReadWriteOnce
      persistentVolumeReclaimPolicy: Recycle
      storageClassName: slow
      mountOptions:
        - hard
        - nfsvers=4.1
      nfs:
        path: /tmp
        server: 172.17.0.2


Capacity        Generally, a PV will have a specific storage capacity. This is 
                set using the PV’s capacity attribute.
                Currently, storage size is the only resource that can be set or 
                requested. Future attributes may include IOPS, throughput, etc.

Volume Mode     Prior to Kubernetes 1.9, all volume plugins created a 
                filesystem on the persistent volume. Now, you can set the value 
                of volumeMode to block to use a raw block device, or filesystem 
                to use a filesystem. filesystem is the default if the value is 
                omitted. This is an optional API parameter.

Access Modes    A PersistentVolume can be mounted on a host in any way 
                supported by the resource provider. As shown in the table 
                below, providers will have different capabilities and each PV’s 
                access modes are set to the specific modes supported by that 
                particular volume. For example, NFS can support multiple 
                read/write clients, but a specific NFS PV might be exported on 
                the server as read-only. Each PV gets its own set of access 
                modes describing that specific PV’s capabilities.

                The access modes are:
                - ReadWriteOnce – the volume can be mounted as read-write by a 
                    single node
                - ReadOnlyMany – the volume can be mounted read-only by many 
                    nodes
                - ReadWriteMany – the volume can be mounted as read-write by 
                    many nodes

                In the CLI, the access modes are abbreviated to:
                - RWO - ReadWriteOnce
                - ROX - ReadOnlyMany
                - RWX - ReadWriteMany

Class           A PV can have a class, which is specified by setting the 
                storageClassName attribute to the name of a StorageClass. A PV 
                of a particular class can only be bound to PVCs requesting that 
                class. A PV with no storageClassName has no class and can only 
                be bound to PVCs that request no particular class.

Reclaim Policy  Current reclaim policies are:
                    - Retain – manual reclamation
                    - Recycle – basic scrub (rm -rf /thevolume/*), valid only 
                        for NFS and HostPath
                    - Delete – volume removal

Mount Options   A Kubernetes administrator can specify additional mount options 
                for when a Persistent Volume is mounted on a node. Not all 
                Persistent Volume types support mount options.
                The following volume types support mount options: CephFS, 
                Glusterfs, NFS, StorageOS (and many others).
                Mount options are not validated, so mount will simply fail if 
                one is invalid.

Node Affinity   You need to explicitly set this field for local volumes.
                A PV can specify node affinity to define constraints that limit 
                what nodes this volume can be accessed from. Pods that use a PV 
                will only be scheduled to nodes that are selected by the node 
                affinity.

Phase           A volume will be in one of the following phases:
                - Available – a free resource that is not yet bound to a claim
                - Bound – the volume is bound to a claim
                - Released – the claim has been deleted, but the resource is 
                    not yet reclaimed by the cluster
                - Failed – the volume has failed its automatic reclamation
                The CLI will show the name of the PVC bound to the PV.


2.5 PersistentVolumeClaims
==========================


Each PVC contains a spec and status, which is the specification and status of 
the claim.

    apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      name: myclaim
    spec:
      accessModes:
        - ReadWriteOnce
    volumeMode: Filesystem
    resources:
      requests:
        storage: 8Gi
    storageClassName: slow
    selector:
      matchLabels:
        release: "stable"
      matchExpressions:
        - {key: environment, operator: In, values: [dev]}


Access Modes    Claims use the same conventions as volumes when requesting 
                storage with specific access modes.

Volume Modes    Claims use the same convention as volumes to indicate the 
                consumption of the volume as either a filesystem or block 
                device.

Resources       Claims, like Pods, can request specific quantities of a 
                resource. In this case, the request is for storage. The same 
                resource model applies to both volumes and claims.

Selector        Claims can specify a label selector to further filter the set 
                of volumes. Only the volumes whose labels match the selector 
                can be bound to the claim. The selector can consist of two 
                fields:
                  - matchLabels - the volume must have a label with this value
                  - matchExpressions - a list of requirements made by 
                      specifying key, list of values, and operator that relates 
                      the key and values. Valid operators include In, NotIn, 
                      Exists, and DoesNotExist.
                All of the requirements, from both matchLabels and 
                matchExpressions, are ANDed together – they must all be 
                satisfied in order to match.

Class           A claim can request a particular class by specifying the name 
                of a StorageClass using the attribute storageClassName. Only 
                PVs of the requested class, ones with the same storageClassName 
                as the PVC, can be bound to the PVC.
                PVCs don’t necessarily have to request a class. A PVC with its 
                storageClassName set equal to "" is always interpreted to be 
                requesting a PV with no class, so it can only be bound to PVs 
                with no class (no annotation or one set equal to ""). A PVC 
                with no storageClassName is not quite the same and is treated 
                differently by the cluster, depending on whether the 
                DefaultStorageClass admission plugin is turned on.
                  - If the admission plugin is turned on, the administrator may 
                      specify a default StorageClass. All PVCs that have no 
                      storageClassName can be bound only to PVs of that 
                      default. Specifying a default StorageClass is done by 
                      setting the annotation 
                      storageclass.kubernetes.io/is-default-class equal to true 
                      in a StorageClass object. If the administrator does not 
                      specify a default, the cluster responds to PVC creation 
                      as if the admission plugin were turned off. If more than 
                      one default is specified, the admission plugin forbids 
                      the creation of all PVCs.
                  - If the admission plugin is turned off, there is no notion 
                      of a default StorageClass. All PVCs that have no 
                      storageClassName can be bound only to PVs that have no 
                      class. In this case, the PVCs that have no 
                      storageClassName are treated the same way as PVCs that 
                      have their storageClassName set to "".
                  Depending on installation method, a default StorageClass may 
                  be deployed to a Kubernetes cluster by addon manager during 
                  installation.
                  When a PVC specifies a selector in addition to requesting a 
                  StorageClass, the requirements are ANDed together: only a PV 
                  of the requested class and with the requested labels may be 
                  bound to the PVC.


2.6 - Claims As Volumes
=======================


Pods access storage by using the claim as a volume. Claims must exist in the 
same namespace as the Pod using the claim. The cluster finds the claim in the 
Pod’s namespace and uses it to get the PersistentVolume backing the claim. The 
volume is then mounted to the host and into the Pod.

    apiVersion: v1
    kind: Pod
    metadata:
      name: mypod
    spec:
      containers:
        - name: myfrontend
          image: nginx
          volumeMounts:
          - mountPath: "/var/www/html"
            name: mypd
      volumes:
        - name: mypd
          persistentVolumeClaim:
            claimName: myclaim

A Note on Namespaces
====================

PersistentVolumes binds are exclusive, and since PersistentVolumeClaims are 
namespaced objects, mounting claims with “Many” modes (ROX, RWX) is only 
possible within one namespace.


2.7 - Raw Block Volume Support
==============================


Out of the volume plugins identified for the tutorial, only the 'Local Volume' 
support raw block volumes.

Persistent Volumes using a Raw Block Volume:

    apiVersion: v1
    kind: PersistentVolume
    metadata:
      name: block-pv
    spec:
      capacity:
        storage: 10Gi
      accessModes:
        - ReadWriteOnce
      volumeMode: Block
      persistentVolumeReclaimPolicy: Retain
      local:
        readOnly: false

Persistent Volume Claim requesting a Raw Block Volume:

    apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      name: block-pvc
    spec:
      accessModes:
        - ReadWriteOnce
      volumeMode: Block
      resources:
        requests:
          storage: 10Gi

Pod specification adding Raw Block Device path in container:

    apiVersion: v1
    kind: Pod
    metadata:
      name: pod-with-block-volume
    spec:
      containers:
        - name: fc-container
          image: fedora:26
          command: ["/bin/sh", "-c"]
          args: [ "tail -f /dev/null" ]
          volumeDevices:
            - name: data
              devicePath: /dev/xvda
      volumes:
        - name: data
          persistentVolumeClaim:
            claimName: block-pvc

    Note: When adding a raw block device for a Pod, you specify the device path 
        in the container instead of a mount path.

   
===================
3 - Storage Classes
===================

This document describes the concept of a StorageClass in Kubernetes. 
Familiarity with volumes and persistent volumes is suggested.

    Introduction
    The StorageClass Resource
    Parameters


3.1 Introduction
================


A StorageClass provides a way for administrators to describe the “classes” of storage they offer. Different classes might map to quality-of-service levels, or to backup policies, or to arbitrary policies determined by the cluster administrators. Kubernetes itself is unopinionated about what classes represent. This concept is sometimes called “profiles” in other storage systems.
The StorageClass Resource

Each StorageClass contains the fields provisioner, parameters, and reclaimPolicy, which are used when a PersistentVolume belonging to the class needs to be dynamically provisioned.

The name of a StorageClass object is significant, and is how users can request a particular class. Administrators set the name and other parameters of a class when first creating StorageClass objects, and the objects cannot be updated once they are created.

Administrators can specify a default StorageClass just for PVCs that don’t request any particular class to bind to: see the PersistentVolumeClaim section for details.

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: standard
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp2
reclaimPolicy: Retain
allowVolumeExpansion: true
mountOptions:
  - debug
volumeBindingMode: Immediate

Provisioner

Storage classes have a provisioner that determines what volume plugin is used for provisioning PVs. This field must be specified.
Volume Plugin	Internal Provisioner	Config Example
AWSElasticBlockStore	✓	AWS EBS
AzureFile	✓	Azure File
AzureDisk	✓	Azure Disk
CephFS	-	-
Cinder	✓	OpenStack Cinder
FC	-	-
FlexVolume	-	-
Flocker	✓	-
GCEPersistentDisk	✓	GCE PD
Glusterfs	✓	Glusterfs
iSCSI	-	-
Quobyte	✓	Quobyte
NFS	-	-
RBD	✓	Ceph RBD
VsphereVolume	✓	vSphere
PortworxVolume	✓	Portworx Volume
ScaleIO	✓	ScaleIO
StorageOS	✓	StorageOS
Local	-	Local

You are not restricted to specifying the “internal” provisioners listed here (whose names are prefixed with “kubernetes.io” and shipped alongside Kubernetes). You can also run and specify external provisioners, which are independent programs that follow a specification defined by Kubernetes. Authors of external provisioners have full discretion over where their code lives, how the provisioner is shipped, how it needs to be run, what volume plugin it uses (including Flex), etc. The repository kubernetes-sigs/sig-storage-lib-external-provisioner houses a library for writing external provisioners that implements the bulk of the specification. Some external provisioners are listed under the repository kubernetes-incubator/external-storage.

For example, NFS doesn’t provide an internal provisioner, but an external provisioner can be used. There are also cases when 3rd party storage vendors provide their own external provisioner.
Reclaim Policy

Persistent Volumes that are dynamically created by a storage class will have the reclaim policy specified in the reclaimPolicy field of the class, which can be either Delete or Retain. If no reclaimPolicy is specified when a StorageClass object is created, it will default to Delete.

Persistent Volumes that are created manually and managed via a storage class will have whatever reclaim policy they were assigned at creation.
Allow Volume Expansion
FEATURE STATE: Kubernetes v1.11 beta

Persistent Volumes can be configured to be expandable. This feature when set to true, allows the users to resize the volume by editing the corresponding PVC object.

The following types of volumes support volume expansion, when the underlying Storage Class has the field allowVolumeExpansion set to true.
Volume type	Required Kubernetes version
gcePersistentDisk	1.11
awsElasticBlockStore	1.11
Cinder	1.11
glusterfs	1.11
rbd	1.11
Azure File	1.11
Azure Disk	1.11
Portworx	1.11
FlexVolume	1.13
CSI	1.14 (alpha), 1.16 (beta)

    Note: You can only use the volume expansion feature to grow a Volume, not to shrink it.

Mount Options

Persistent Volumes that are dynamically created by a storage class will have the mount options specified in the mountOptions field of the class.

If the volume plugin does not support mount options but mount options are specified, provisioning will fail. Mount options are not validated on either the class or PV, so mount of the PV will simply fail if one is invalid.
Volume Binding Mode

The volumeBindingMode field controls when volume binding and dynamic provisioning should occur.

By default, the Immediate mode indicates that volume binding and dynamic provisioning occurs once the PersistentVolumeClaim is created. For storage backends that are topology-constrained and not globally accessible from all Nodes in the cluster, PersistentVolumes will be bound or provisioned without knowledge of the Pod’s scheduling requirements. This may result in unschedulable Pods.

A cluster administrator can address this issue by specifying the WaitForFirstConsumer mode which will delay the binding and provisioning of a PersistentVolume until a Pod using the PersistentVolumeClaim is created. PersistentVolumes will be selected or provisioned conforming to the topology that is specified by the Pod’s scheduling constraints. These include, but are not limited to, resource requirements, node selectors, pod affinity and anti-affinity, and taints and tolerations.

The following plugins support WaitForFirstConsumer with dynamic provisioning:

    AWSElasticBlockStore
    GCEPersistentDisk
    AzureDisk

The following plugins support WaitForFirstConsumer with pre-created PersistentVolume binding:

    All of the above
    Local

FEATURE STATE: Kubernetes 1.17 stable

CSI volumes are also supported with dynamic provisioning and pre-created PVs, but you’ll need to look at the documentation for a specific CSI driver to see its supported topology keys and examples.
Allowed Topologies

When a cluster operator specifies the WaitForFirstConsumer volume binding mode, it is no longer necessary to restrict provisioning to specific topologies in most situations. However, if still required, allowedTopologies can be specified.

This example demonstrates how to restrict the topology of provisioned volumes to specific zones and should be used as a replacement for the zone and zones parameters for the supported plugins.

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: standard
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-standard
volumeBindingMode: WaitForFirstConsumer
allowedTopologies:
- matchLabelExpressions:
  - key: failure-domain.beta.kubernetes.io/zone
    values:
    - us-central1-a
    - us-central1-b

Parameters

Storage classes have parameters that describe volumes belonging to the storage class. Different parameters may be accepted depending on the provisioner. For example, the value io1, for the parameter type, and the parameter iopsPerGB are specific to EBS. When a parameter is omitted, some default is used.
AWS EBS

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: slow
provisioner: kubernetes.io/aws-ebs
parameters:
  type: io1
  iopsPerGB: "10"
  fsType: ext4

    type: io1, gp2, sc1, st1. See AWS docs for details. Default: gp2.
    zone (Deprecated): AWS zone. If neither zone nor zones is specified, volumes are generally round-robin-ed across all active zones where Kubernetes cluster has a node. zone and zones parameters must not be used at the same time.
    zones (Deprecated): A comma separated list of AWS zone(s). If neither zone nor zones is specified, volumes are generally round-robin-ed across all active zones where Kubernetes cluster has a node. zone and zones parameters must not be used at the same time.
    iopsPerGB: only for io1 volumes. I/O operations per second per GiB. AWS volume plugin multiplies this with size of requested volume to compute IOPS of the volume and caps it at 20 000 IOPS (maximum supported by AWS, see AWS docs. A string is expected here, i.e. "10", not 10.
    fsType: fsType that is supported by kubernetes. Default: "ext4".
    encrypted: denotes whether the EBS volume should be encrypted or not. Valid values are "true" or "false". A string is expected here, i.e. "true", not true.
    kmsKeyId: optional. The full Amazon Resource Name of the key to use when encrypting the volume. If none is supplied but encrypted is true, a key is generated by AWS. See AWS docs for valid ARN value.

    Note: zone and zones parameters are deprecated and replaced with allowedTopologies

GCE PD

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: slow
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-standard
  replication-type: none

    type: pd-standard or pd-ssd. Default: pd-standard
    zone (Deprecated): GCE zone. If neither zone nor zones is specified, volumes are generally round-robin-ed across all active zones where Kubernetes cluster has a node. zone and zones parameters must not be used at the same time.
    zones (Deprecated): A comma separated list of GCE zone(s). If neither zone nor zones is specified, volumes are generally round-robin-ed across all active zones where Kubernetes cluster has a node. zone and zones parameters must not be used at the same time.
    replication-type: none or regional-pd. Default: none.

If replication-type is set to none, a regular (zonal) PD will be provisioned.

If replication-type is set to regional-pd, a Regional Persistent Disk will be provisioned. In this case, users must use zones instead of zone to specify the desired replication zones. If exactly two zones are specified, the Regional PD will be provisioned in those zones. If more than two zones are specified, Kubernetes will arbitrarily choose among the specified zones. If the zones parameter is omitted, Kubernetes will arbitrarily choose among zones managed by the cluster.

    Note: zone and zones parameters are deprecated and replaced with allowedTopologies

Glusterfs

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: slow
provisioner: kubernetes.io/glusterfs
parameters:
  resturl: "http://127.0.0.1:8081"
  clusterid: "630372ccdc720a92c681fb928f27b53f"
  restauthenabled: "true"
  restuser: "admin"
  secretNamespace: "default"
  secretName: "heketi-secret"
  gidMin: "40000"
  gidMax: "50000"
  volumetype: "replicate:3"

    resturl: Gluster REST service/Heketi service url which provision gluster volumes on demand. The general format should be IPaddress:Port and this is a mandatory parameter for GlusterFS dynamic provisioner. If Heketi service is exposed as a routable service in openshift/kubernetes setup, this can have a format similar to http://heketi-storage-project.cloudapps.mystorage.com where the fqdn is a resolvable Heketi service url.
    restauthenabled : Gluster REST service authentication boolean that enables authentication to the REST server. If this value is "true", restuser and restuserkey or secretNamespace + secretName have to be filled. This option is deprecated, authentication is enabled when any of restuser, restuserkey, secretName or secretNamespace is specified.
    restuser : Gluster REST service/Heketi user who has access to create volumes in the Gluster Trusted Pool.
    restuserkey : Gluster REST service/Heketi user’s password which will be used for authentication to the REST server. This parameter is deprecated in favor of secretNamespace + secretName.

    secretNamespace, secretName : Identification of Secret instance that contains user password to use when talking to Gluster REST service. These parameters are optional, empty password will be used when both secretNamespace and secretName are omitted. The provided secret must have type "kubernetes.io/glusterfs", e.g. created in this way:

    kubectl create secret generic heketi-secret \
      --type="kubernetes.io/glusterfs" --from-literal=key='opensesame' \
      --namespace=default

    Example of a secret can be found in glusterfs-provisioning-secret.yaml.

    clusterid: 630372ccdc720a92c681fb928f27b53f is the ID of the cluster which will be used by Heketi when provisioning the volume. It can also be a list of clusterids, for example: "8452344e2becec931ece4e33c4674e4e,42982310de6c63381718ccfa6d8cf397". This is an optional parameter.

    gidMin, gidMax : The minimum and maximum value of GID range for the storage class. A unique value (GID) in this range ( gidMin-gidMax ) will be used for dynamically provisioned volumes. These are optional values. If not specified, the volume will be provisioned with a value between 2000-2147483647 which are defaults for gidMin and gidMax respectively.

    volumetype : The volume type and its parameters can be configured with this optional value. If the volume type is not mentioned, it’s up to the provisioner to decide the volume type.

    For example:
        Replica volume: volumetype: replicate:3 where ‘3’ is replica count.
        Disperse/EC volume: volumetype: disperse:4:2 where ‘4’ is data and ‘2’ is the redundancy count.
        Distribute volume: volumetype: none

    For available volume types and administration options, refer to the Administration Guide.

    For further reference information, see How to configure Heketi.

    When persistent volumes are dynamically provisioned, the Gluster plugin automatically creates an endpoint and a headless service in the name gluster-dynamic-<claimname>. The dynamic endpoint and service are automatically deleted when the persistent volume claim is deleted.

OpenStack Cinder

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: gold
provisioner: kubernetes.io/cinder
parameters:
  availability: nova

    availability: Availability Zone. If not specified, volumes are generally round-robin-ed across all active zones where Kubernetes cluster has a node.

    Note:
    FEATURE STATE: Kubernetes 1.11 deprecated

    This internal provisioner of OpenStack is deprecated. Please use the external cloud provider for OpenStack.

vSphere

    Create a StorageClass with a user specified disk format.

    apiVersion: storage.k8s.io/v1
    kind: StorageClass
    metadata:
      name: fast
    provisioner: kubernetes.io/vsphere-volume
    parameters:
      diskformat: zeroedthick

    diskformat: thin, zeroedthick and eagerzeroedthick. Default: "thin".

    Create a StorageClass with a disk format on a user specified datastore.

    apiVersion: storage.k8s.io/v1
    kind: StorageClass
    metadata:
      name: fast
    provisioner: kubernetes.io/vsphere-volume
    parameters:
        diskformat: zeroedthick
        datastore: VSANDatastore

    datastore: The user can also specify the datastore in the StorageClass. The volume will be created on the datastore specified in the storage class, which in this case is VSANDatastore. This field is optional. If the datastore is not specified, then the volume will be created on the datastore specified in the vSphere config file used to initialize the vSphere Cloud Provider.

    Storage Policy Management inside kubernetes

        Using existing vCenter SPBM policy

        One of the most important features of vSphere for Storage Management is policy based Management. Storage Policy Based Management (SPBM) is a storage policy framework that provides a single unified control plane across a broad range of data services and storage solutions. SPBM enables vSphere administrators to overcome upfront storage provisioning challenges, such as capacity planning, differentiated service levels and managing capacity headroom.

        The SPBM policies can be specified in the StorageClass using the storagePolicyName parameter.

        Virtual SAN policy support inside Kubernetes

        Vsphere Infrastructure (VI) Admins will have the ability to specify custom Virtual SAN Storage Capabilities during dynamic volume provisioning. You can now define storage requirements, such as performance and availability, in the form of storage capabilities during dynamic volume provisioning. The storage capability requirements are converted into a Virtual SAN policy which are then pushed down to the Virtual SAN layer when a persistent volume (virtual disk) is being created. The virtual disk is distributed across the Virtual SAN datastore to meet the requirements.

        You can see Storage Policy Based Management for dynamic provisioning of volumes for more details on how to use storage policies for persistent volumes management.

There are few vSphere examples which you try out for persistent volume management inside Kubernetes for vSphere.
Ceph RBD

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast
provisioner: kubernetes.io/rbd
parameters:
  monitors: 10.16.153.105:6789
  adminId: kube
  adminSecretName: ceph-secret
  adminSecretNamespace: kube-system
  pool: kube
  userId: kube
  userSecretName: ceph-secret-user
  userSecretNamespace: default
  fsType: ext4
  imageFormat: "2"
  imageFeatures: "layering"

    monitors: Ceph monitors, comma delimited. This parameter is required.
    adminId: Ceph client ID that is capable of creating images in the pool. Default is “admin”.
    adminSecretName: Secret Name for adminId. This parameter is required. The provided secret must have type “kubernetes.io/rbd”.
    adminSecretNamespace: The namespace for adminSecretName. Default is “default”.
    pool: Ceph RBD pool. Default is “rbd”.
    userId: Ceph client ID that is used to map the RBD image. Default is the same as adminId.

    userSecretName: The name of Ceph Secret for userId to map RBD image. It must exist in the same namespace as PVCs. This parameter is required. The provided secret must have type “kubernetes.io/rbd”, e.g. created in this way:

    kubectl create secret generic ceph-secret --type="kubernetes.io/rbd" \
      --from-literal=key='QVFEQ1pMdFhPUnQrSmhBQUFYaERWNHJsZ3BsMmNjcDR6RFZST0E9PQ==' \
      --namespace=kube-system

    userSecretNamespace: The namespace for userSecretName.

    fsType: fsType that is supported by kubernetes. Default: "ext4".

    imageFormat: Ceph RBD image format, “1” or “2”. Default is “2”.

    imageFeatures: This parameter is optional and should only be used if you set imageFormat to “2”. Currently supported features are layering only. Default is “”, and no features are turned on.

Quobyte

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: slow
provisioner: kubernetes.io/quobyte
parameters:
    quobyteAPIServer: "http://138.68.74.142:7860"
    registry: "138.68.74.142:7861"
    adminSecretName: "quobyte-admin-secret"
    adminSecretNamespace: "kube-system"
    user: "root"
    group: "root"
    quobyteConfig: "BASE"
    quobyteTenant: "DEFAULT"

    quobyteAPIServer: API Server of Quobyte in the format "http(s)://api-server:7860"
    registry: Quobyte registry to use to mount the volume. You can specify the registry as <host>:<port> pair or if you want to specify multiple registries you just have to put a comma between them e.q. <host1>:<port>,<host2>:<port>,<host3>:<port>. The host can be an IP address or if you have a working DNS you can also provide the DNS names.
    adminSecretNamespace: The namespace for adminSecretName. Default is “default”.

    adminSecretName: secret that holds information about the Quobyte user and the password to authenticate against the API server. The provided secret must have type “kubernetes.io/quobyte” and the keys user and password, e.g. created in this way:

    kubectl create secret generic quobyte-admin-secret \
      --type="kubernetes.io/quobyte" --from-literal=user='admin' --from-literal=password='opensesame' \
      --namespace=kube-system

    user: maps all access to this user. Default is “root”.

    group: maps all access to this group. Default is “nfsnobody”.

    quobyteConfig: use the specified configuration to create the volume. You can create a new configuration or modify an existing one with the Web console or the quobyte CLI. Default is “BASE”.

    quobyteTenant: use the specified tenant ID to create/delete the volume. This Quobyte tenant has to be already present in Quobyte. Default is “DEFAULT”.

Azure Disk
Azure Unmanaged Disk Storage Class

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: slow
provisioner: kubernetes.io/azure-disk
parameters:
  skuName: Standard_LRS
  location: eastus
  storageAccount: azure_storage_account_name

    skuName: Azure storage account Sku tier. Default is empty.
    location: Azure storage account location. Default is empty.
    storageAccount: Azure storage account name. If a storage account is provided, it must reside in the same resource group as the cluster, and location is ignored. If a storage account is not provided, a new storage account will be created in the same resource group as the cluster.

New Azure Disk Storage Class (starting from v1.7.2)

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: slow
provisioner: kubernetes.io/azure-disk
parameters:
  storageaccounttype: Standard_LRS
  kind: Shared

    storageaccounttype: Azure storage account Sku tier. Default is empty.
    kind: Possible values are shared (default), dedicated, and managed. When kind is shared, all unmanaged disks are created in a few shared storage accounts in the same resource group as the cluster. When kind is dedicated, a new dedicated storage account will be created for the new unmanaged disk in the same resource group as the cluster. When kind is managed, all managed disks are created in the same resource group as the cluster.

    resourceGroup: Specify the resource group in which the Azure disk will be created. It must be an existing resource group name. If it is unspecified, the disk will be placed in the same resource group as the current Kubernetes cluster.

    Premium VM can attach both Standard_LRS and Premium_LRS disks, while Standard VM can only attach Standard_LRS disks.

    Managed VM can only attach managed disks and unmanaged VM can only attach unmanaged disks.

Azure File

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: azurefile
provisioner: kubernetes.io/azure-file
parameters:
  skuName: Standard_LRS
  location: eastus
  storageAccount: azure_storage_account_name

    skuName: Azure storage account Sku tier. Default is empty.
    location: Azure storage account location. Default is empty.
    storageAccount: Azure storage account name. Default is empty. If a storage account is not provided, all storage accounts associated with the resource group are searched to find one that matches skuName and location. If a storage account is provided, it must reside in the same resource group as the cluster, and skuName and location are ignored.
    secretNamespace: the namespace of the secret that contains the Azure Storage Account Name and Key. Default is the same as the Pod.
    secretName: the name of the secret that contains the Azure Storage Account Name and Key. Default is azure-storage-account-<accountName>-secret
    readOnly: a flag indicating whether the storage will be mounted as read only. Defaults to false which means a read/write mount. This setting will impact the ReadOnly setting in VolumeMounts as well.

During storage provisioning, a secret named by secretName is created for the mounting credentials. If the cluster has enabled both RBAC and Controller Roles, add the create permission of resource secret for clusterrole system:controller:persistent-volume-binder.

In a multi-tenancy context, it is strongly recommended to set the value for secretNamespace explicitly, otherwise the storage account credentials may be read by other users.
Portworx Volume

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: portworx-io-priority-high
provisioner: kubernetes.io/portworx-volume
parameters:
  repl: "1"
  snap_interval:   "70"
  io_priority:  "high"

    fs: filesystem to be laid out: none/xfs/ext4 (default: ext4).
    block_size: block size in Kbytes (default: 32).
    repl: number of synchronous replicas to be provided in the form of replication factor 1..3 (default: 1) A string is expected here i.e. "1" and not 1.
    io_priority: determines whether the volume will be created from higher performance or a lower priority storage high/medium/low (default: low).
    snap_interval: clock/time interval in minutes for when to trigger snapshots. Snapshots are incremental based on difference with the prior snapshot, 0 disables snaps (default: 0). A string is expected here i.e. "70" and not 70.
    aggregation_level: specifies the number of chunks the volume would be distributed into, 0 indicates a non-aggregated volume (default: 0). A string is expected here i.e. "0" and not 0
    ephemeral: specifies whether the volume should be cleaned-up after unmount or should be persistent. emptyDir use case can set this value to true and persistent volumes use case such as for databases like Cassandra should set to false, true/false (default false). A string is expected here i.e. "true" and not true.

ScaleIO

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: slow
provisioner: kubernetes.io/scaleio
parameters:
  gateway: https://192.168.99.200:443/api
  system: scaleio
  protectionDomain: pd0
  storagePool: sp1
  storageMode: ThinProvisioned
  secretRef: sio-secret
  readOnly: false
  fsType: xfs

    provisioner: attribute is set to kubernetes.io/scaleio
    gateway: address to a ScaleIO API gateway (required)
    system: the name of the ScaleIO system (required)
    protectionDomain: the name of the ScaleIO protection domain (required)
    storagePool: the name of the volume storage pool (required)
    storageMode: the storage provision mode: ThinProvisioned (default) or ThickProvisioned
    secretRef: reference to a configured Secret object (required)
    readOnly: specifies the access mode to the mounted volume (default false)
    fsType: the file system to use for the volume (default ext4)

The ScaleIO Kubernetes volume plugin requires a configured Secret object. The secret must be created with type kubernetes.io/scaleio and use the same namespace value as that of the PVC where it is referenced as shown in the following command:

kubectl create secret generic sio-secret --type="kubernetes.io/scaleio" \
--from-literal=username=sioadmin --from-literal=password=d2NABDNjMA== \
--namespace=default

StorageOS

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast
provisioner: kubernetes.io/storageos
parameters:
  pool: default
  description: Kubernetes volume
  fsType: ext4
  adminSecretNamespace: default
  adminSecretName: storageos-secret

    pool: The name of the StorageOS distributed capacity pool to provision the volume from. Uses the default pool which is normally present if not specified.
    description: The description to assign to volumes that were created dynamically. All volume descriptions will be the same for the storage class, but different storage classes can be used to allow descriptions for different use cases. Defaults to Kubernetes volume.
    fsType: The default filesystem type to request. Note that user-defined rules within StorageOS may override this value. Defaults to ext4.
    adminSecretNamespace: The namespace where the API configuration secret is located. Required if adminSecretName set.
    adminSecretName: The name of the secret to use for obtaining the StorageOS API credentials. If not specified, default values will be attempted.

The StorageOS Kubernetes volume plugin can use a Secret object to specify an endpoint and credentials to access the StorageOS API. This is only required when the defaults have been changed. The secret must be created with type kubernetes.io/storageos as shown in the following command:

kubectl create secret generic storageos-secret \
--type="kubernetes.io/storageos" \
--from-literal=apiAddress=tcp://localhost:5705 \
--from-literal=apiUsername=storageos \
--from-literal=apiPassword=storageos \
--namespace=default

Secrets used for dynamically provisioned volumes may be created in any namespace and referenced with the adminSecretNamespace parameter. Secrets used by pre-provisioned volumes must be created in the same namespace as the PVC that references it.
Local
FEATURE STATE: Kubernetes v1.14 stable

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer

Local volumes do not currently support dynamic provisioning, however a StorageClass should still be created to delay volume binding until pod scheduling. This is specified by the WaitForFirstConsumer volume binding mode.

Delaying volume binding allows the scheduler to consider all of a pod’s scheduling constraints when choosing an appropriate PersistentVolume for a PersistentVolumeClaim.



================================
4 - Dynamic Volumes Provisioning
================================


Dynamic volume provisioning allows storage volumes to be created on-demand. Without dynamic provisioning, cluster administrators have to manually make calls to their cloud or storage provider to create new storage volumes, and then create PersistentVolume objects to represent them in Kubernetes. The dynamic provisioning feature eliminates the need for cluster administrators to pre-provision storage. Instead, it automatically provisions storage when it is requested by users.

    Background
    Enabling Dynamic Provisioning
    Using Dynamic Provisioning
    Defaulting Behavior
    Topology Awareness

Background

The implementation of dynamic volume provisioning is based on the API object StorageClass from the API group storage.k8s.io. A cluster administrator can define as many StorageClass objects as needed, each specifying a volume plugin (aka provisioner) that provisions a volume and the set of parameters to pass to that provisioner when provisioning. A cluster administrator can define and expose multiple flavors of storage (from the same or different storage systems) within a cluster, each with a custom set of parameters. This design also ensures that end users don’t have to worry about the complexity and nuances of how storage is provisioned, but still have the ability to select from multiple storage options.

More information on storage classes can be found here.
Enabling Dynamic Provisioning

To enable dynamic provisioning, a cluster administrator needs to pre-create one or more StorageClass objects for users. StorageClass objects define which provisioner should be used and what parameters should be passed to that provisioner when dynamic provisioning is invoked. The following manifest creates a storage class “slow” which provisions standard disk-like persistent disks.

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: slow
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-standard

The following manifest creates a storage class “fast” which provisions SSD-like persistent disks.

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-ssd

Using Dynamic Provisioning

Users request dynamically provisioned storage by including a storage class in their PersistentVolumeClaim. Before Kubernetes v1.6, this was done via the volume.beta.kubernetes.io/storage-class annotation. However, this annotation is deprecated since v1.6. Users now can and should instead use the storageClassName field of the PersistentVolumeClaim object. The value of this field must match the name of a StorageClass configured by the administrator (see below).

To select the “fast” storage class, for example, a user would create the following PersistentVolumeClaim:

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: claim1
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: fast
  resources:
    requests:
      storage: 30Gi

This claim results in an SSD-like Persistent Disk being automatically provisioned. When the claim is deleted, the volume is destroyed.
Defaulting Behavior

Dynamic provisioning can be enabled on a cluster such that all claims are dynamically provisioned if no storage class is specified. A cluster administrator can enable this behavior by:

    Marking one StorageClass object as default;
    Making sure that the DefaultStorageClass admission controller is enabled on the API server.

An administrator can mark a specific StorageClass as default by adding the storageclass.kubernetes.io/is-default-class annotation to it. When a default StorageClass exists in a cluster and a user creates a PersistentVolumeClaim with storageClassName unspecified, the DefaultStorageClass admission controller automatically adds the storageClassName field pointing to the default storage class.

Note that there can be at most one default storage class on a cluster, or a PersistentVolumeClaim without storageClassName explicitly specified cannot be created.
Topology Awareness

In Multi-Zone clusters, Pods can be spread across Zones in a Region. Single-Zone storage backends should be provisioned in the Zones where Pods are scheduled. This can be accomplished by setting the Volume Binding Mode.





