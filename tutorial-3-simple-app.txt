#############################################################################
#                                                                           #
# Learn Kubernetes Basics - Part 3 - Deploy a simple app                    #
#                                                                           #
#############################################################################


1 - Kubernetes Deployments
==========================

Once you have a running Kubernetes cluster, you can deploy your containerized 
applications on top of it. To deploy your containerized application on top of 
the cluster, you create what is called a "Kubernetes Deployment". The 
Deployment maerializes through a text file (yet another a YAML file) which 
defines the target state of your application (which application - i.e. which 
Docker container or which set of containers - will compose your application, on 
how many nodes - in order to bring resilience - or other criterias your 
application should respect once it is actually deployed on the cluster). This 
text file instructs Kubernetes how to create and update instances of your 
application (actually, it tells the Kubernetes Controller Manager to spawn a 
Deployment Controller which will read this text file: the Deployment Controller 
will tell the Kubernetes master how to 'schedules' the pods onto individual 
Nodes in the cluster, each pod carrying the containers composing your 
application).

Once the application instances are created, the Kubernetes Deployment 
Controller continuously monitors those instances. If the Node hosting an 
instance goes down or is deleted, the Deployment controller replaces the 
instance with an instance on another Node in the cluster. This provides a 
self-healing mechanism to address machine failure or maintenance: since the 
text file describes the desired target state of the deployment, the Deployment 
controller actually detects that there is a deviation of the real deployment 
vs. the described target, and it reacts by deploying more pods to available 
nodes.

In a pre-orchestration world, installation scripts would often be used to start 
applications, but they did not allow recovery from machine failure. By both 
creating your application instances and keeping them running across Nodes, 
Kubernetes Deployments provide a fundamentally different approach to 
application management.


2 - Deploying your first app on Kubernetes
==========================================

You can create and manage a Deployment by using the Kubernetes command line 
interface, Kubectl. Kubectl uses the Kubernetes API to interact with the 
cluster. In this module, you'll learn the most common Kubectl commands needed 
to create Deployments that run your applications on a Kubernetes cluster.

When you create a Deployment, you'll need to specify the container image for 
your application and the number of replicas that you want to run. You can 
change that information later by updating your Deployment (i.e. updating the 
YAML file); sections 5 and 6 of this tutorial discuss how you can scale and 
update your Deployments.

Applications need to be packaged into one of the supported container formats 
in order to be deployed on Kubernetes: here we will use Docker.

For your first Deployment, you'll use a Node.js application packaged in a 
Docker container. The image is provided in the Kubernetes official site
(https://kubernetes.io/docs/tutorials/hello-minikube/), so we don't have to 
actually code a new application for our tutorial. The corresponding node.js 
script and the dockerfile are available in the "hello-world" directory if you
want to re-build your local image.

Let’s deploy our first app on Kubernetes with the 'kubectl create deployment'
command. We need to provide the deployment name and app image location 
(include the full repository url for images hosted outside Docker hub).

tso@laptop:~$ kubectl create deployment hello --image=gcr.io/google-samples/node-hello:1.0 
deployment.apps/hello created

Great! You just deployed your first application by creating a deployment 
(which you can also see in the dashboard (image: 
'dashboard-deployment-first'). You can also see that it created a Pod called 
'hello-7747bc55f-p486h' as it is visible on the dashboard 
(image: 'dashboard-pod-bootcamp').

You can see it from the terminal:

tso@laptop:/projects/kind$ kubectl get pods -o wide
NAME                    READY   STATUS    RESTARTS   AGE    IP           NODE              NOMINATED NODE   READINESS GATES
hello-7747bc55f-2t4tb   1/1     Running   0          2m3s   10.244.1.2   newyear-worker2   <none>           <none>


    Note: from now on, we assume that you will go frequently on the dashboard 
        to check the changes happening on the cluster, and we will focus the 
        tutorial on the CLI commadns and results as they appear in the 
        terminal. Again, the purpose is solely to get you aquainted with the 
        Kubernetes concepts and to manipulate the cluster, while you may get 
        far more efficient at manipulating directly the REST APIs or managing 
        some actions via the dashboard itself.
        So, as of now, we will not mention directly screenshots related to the 
        progress on the tutorial, but you may see more screenshots in the 
        'images' directory.

This 'create deployment' command performed a few things for you:

    - searched for a suitable node where an instance of the application could 
      be run (we have only 1 available node)
    - scheduled the application to run on that Node
    - configured the cluster to reschedule the instance on a new Node when 
      needed

To list your deployments use the get deployments command:

tso@laptop:/projects/kind$ kubectl get deployments
NAME    READY   UP-TO-DATE   AVAILABLE   AGE
hello   1/1     1            1           2m33s


Let's details this line since you will see later in the tutorial many results 
similar to this one:
    - The 1/1 means '1 ready for 1 Desired': 1 ready is the number of pod 
        active (with  the container which is mentioned in the YAML file) and 
        the YAML file specifies that we should always have 1 instance running.
    - The 1 UP-TO-DATE means that the one active is of the right version: 
        there is consequently no need for Kubernetes to update the Pod with a 
        fresher version.
    - The 1 AVAILABLE means that 1 isntance of the Pod is actually available 
        to the end-users (and there could be many reasons for which the Pod 
        would not be available: for instance, the network could be down for a 
        part of the cluster, thus the corresponding Node would beisolated, and 
        Kubernetes would have to spawn a new instance of the Pod on another 
        Node with good conenctivity, in order to secure that the end-users 
        keep having 1 instance truely available to them).

We can see here that there is 1 deployment, running 1 single instance of your 
app. The instance is running inside a Docker container on one of the nodes. To 
get more details, we expand the results of the 'get pods' command: we can see 
that this Pod is running on the slave 1.

tso@laptop:/projects/kind$ kubectl get pods -o wide
NAME                    READY   STATUS    RESTARTS   AGE    IP           NODE          NOMINATED NODE   READINESS GATES
hello-7747bc55f-2t4tb   1/1     Running   0          2m3s   10.244.1.2   newyear-worker2   <none>           <none>



3 - Connecting to your app from within the cluster
==================================================

Pods that are running inside Kubernetes are running on a private, isolated 
network. By default they are visible from other pods and services within the 
same kubernetes cluster, but not outside that network. When we use kubectl, 
we're interacting through an API endpoint to communicate with our application.

We will cover other options on how to expose your application outside the 
kubernetes cluster in the later section. For the moment, we will still use the 
proxy that will forward communications into the cluster-wide, private network. 
The proxy can be terminated by pressing control-C and won't show any output 
while its running.

If you terminated the proxy, we will restart it in a second terminal tab:

tso@laptop:~$ gnome-terminal bash --tab -- kubectl proxy -p 8080
Starting to serve on 127.0.0.1:8080

The proxy enables direct access to the API from these terminals: you can see 
all those APIs hosted through the proxy endpoint. For example, we can query the 
version directly through the API using the curl command:

tso@laptop:~$ curl http://localhost:8080/version
{
  "major": "1",
  "minor": "16",
  "gitVersion": "v1.16.3",
  "gitCommit": "b3cbbae08ec52a7fc73d334838e18d17e8512749",
  "gitTreeState": "clean",
  "buildDate": "2019-12-04T07:23:47Z",
  "goVersion": "go1.12.12",
  "compiler": "gc",
  "platform": "linux/amd64"
}

If Port 8080 is not accessible, ensure that the kubectl proxy started above is 
running.

The API server will automatically create an endpoint for each pod, based on 
the pod name, that is also accessible through the proxy.

First we need to get the Pod name, and we'll store in the environment variable 
POD_NAME:

tso@laptop:~$ export POD_NAME=$(kubectl get pods -o go-template --template '{{range .items}}{{.metadata.name}}{{"\n"}}{{end}}')
tso@laptop:~$ echo $POD_NAME
hello-7747bc55f-2t4tb

In order for the new deployment to be accessible without using the Proxy, a 
'Service' is required which will be explained in the next modules.


4 - Explore your app
====================


4.1 - Kubernetes Pods
=====================

When you created a Deployment in Section 2, Kubernetes created a Pod to host 
your application instance. A Pod is a Kubernetes abstraction that represents a 
group of one or more application containers (such as Docker or rkt), and some 
shared resources for those containers. Those resources include:

    - Shared storage, as Volumes
    - Networking, as a unique cluster IP address
    - Information about how to run each container, such as the container image 
        version or specific ports to use

A Pod models an application-specific "logical host" and can contain different 
application containers which are relatively tightly coupled. For example, a 
Pod might include both the container with your Node.js app as well as a 
different container that feeds the data to be published by the Node.js 
webserver. The containers in a Pod share the same IP Address and port space, 
are always co-located and co-scheduled, and run in a shared context on the 
same Node.

Pods are the atomic unit on the Kubernetes platform. When we create a 
Deployment on Kubernetes, that Deployment creates Pods with containers inside 
them (as opposed to creating containers directly). Each Pod is tied to the 
Node where it is scheduled, and remains there until termination (according to 
restart policy) or deletion. In case of a Node failure, identical Pods are 
scheduled on other available Nodes in the cluster.

Pods overview (image "view your app - pod overview")

4.2 - Nodes
===========

A Pod always runs on a Node. A Node is a worker machine in Kubernetes and may 
be either a virtual or a physical machine, depending on the cluster. Each Node 
is managed by the Master. A Node can have multiple pods, and the Kubernetes 
Master automatically handles scheduling the pods across the Nodes in the 
cluster. The Master's automatic scheduling takes into account the available 
resources on each Node.

Every Kubernetes Node runs at least:

    - Kubelet, a process responsible for communication between the Kubernetes 
        Master and the Node; it manages the Pods and the containers running on 
        a machine.
    - A container runtime (like Docker, rkt) responsible for pulling the 
        container image from a registry, unpacking the container, and running 
        the application.

Containers should only be scheduled together in a single Pod if they are 
tightly coupled and need to share resources such as disk.

Node overview (image "view your app - node overview")


4.3 - Check the application configuration
=========================================

We already have checked the pods with kubectl, so we know that a 
kubernetes-bootcamp pod runs on the slave 2. Now, let's view what containers 
are inside that Pod and what images are used to build those containers. To do 
so, we run the describe pods command:

tso@laptop:~$ kubectl describe pods
Name:         hello-7747bc55f-2t4tb
Namespace:    default
Priority:     0
Node:         newyear-worker2/172.17.0.2
Start Time:   Tue, 31 Dec 2019 15:28:15 +0100
Labels:       app=hello
              pod-template-hash=7747bc55f
Annotations:  <none>
Status:       Running
IP:           10.244.1.2
IPs:
  IP:           10.244.1.2
Controlled By:  ReplicaSet/hello-7747bc55f
Containers:
  node-hello:
    Container ID:   containerd://61562b770e3723a442af35e32d15998c01de8da7fa6eeaf49058dcc9c31b5f63
    Image:          gcr.io/google-samples/node-hello:1.0
    Image ID:       sha256:9ef4b4c241fc28ce894a57a7cb218b796838fdccf15515d784791abc2ffc1ee6
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 31 Dec 2019 15:28:38 +0100
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-ltwvj (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-ltwvj:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-ltwvj
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type    Reason     Age    From                      Message
  ----    ------     ----   ----                      -------
  Normal  Scheduled  5m18s  default-scheduler         Successfully assigned default/hello-7747bc55f-2t4tb to newyear-worker2
  Normal  Pulling    5m18s  kubelet, newyear-worker2  Pulling image "gcr.io/google-samples/node-hello:1.0"
  Normal  Pulled     4m55s  kubelet, newyear-worker2  Successfully pulled image "gcr.io/google-samples/node-hello:1.0"
  Normal  Created    4m55s  kubelet, newyear-worker2  Created container node-hello
  Normal  Started    4m55s  kubelet, newyear-worker2  Started container node-hello


Whaou... Plenty of information is available, as you can see: IP address, the 
ports used and a list of events related to the lifecycle of the Pod.

The output of the describe command is extensive and covers some concepts that 
we didn’t explain yet, but don’t worry, they will become familiar by the end 
of this bootcamp.

    Note: the describe command can be used to get detailed information about 
        most of the kubernetes primitives: node, pods, deployments. The 
        describe output is designed to be human readable, not to be scripted 
        against.


4.4 - Show the app in the terminal
===================================

Recall that Pods are running in an isolated, private network - so we continue 
with the kubectl proxy command in a second terminal window (on prt 8080). 

You have store the Pod name in the POD_NAME environment variable.

To see the output of our application, run a curl request.

tso@laptop:~$ kubectl get namespace
NAME                   STATUS   AGE
default                Active   10m
kube-node-lease        Active   10m
kube-public            Active   10m
kube-system            Active   10m
kubernetes-dashboard   Active   8m55s

tso@laptop:~$ echo $POD_NAME
hello-7747bc55f-2t4tb
tso@laptop:~$ curl http://localhost:8080/api/v1/namespace/default/pods/$POD_NAME/
{
  "kind": "Status",
  "apiVersion": "v1",
  "metadata": {
    
  },
  "status": "Failure",
  "message": "the server could not find the requested resource",
  "reason": "NotFound",
  "details": {
    
  },
  "code": 404
}

Arg... Can't see why it does not show the details, while everything is 
available on the dashboard... Since I get a lot of details when polling the 
'pods' URL (curl http://localhost:8080/api/v1/pods/), I guess htat this is 
related to not having the right API signature...

Let's try it from another angle: we first list the pods and grep the 'hello' 
tag from it. There is an interesting field called 'selfLink': it enables to get 
directly to the desired resource.

tso@laptop:~$ curl http://localhost:8080/api/v1/pods/ | grep hello
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0        "name": "hello-7747bc55f-2t4tb",
        "generateName": "hello-7747bc55f-",
        "selfLink": "/api/v1/namespaces/default/pods/hello-7747bc55f-2t4tb",
          "app": "hello",
            "name": "hello-7747bc55f",
            "name": "node-hello",
            "image": "gcr.io/google-samples/node-hello:1.0",
            "name": "node-hello",
            "image": "gcr.io/google-samples/node-hello:1.0",
100 98274    0 98274    0     0  10.4M      0 --:--:-- --:--:-- --:--:-- 10.4M

So we will now poll specifically this URL:

tso@laptop:~$ curl http://localhost:8080/api/v1/namespaces/default/pods/hello-7747bc55f-2t4tb
{
  "kind": "Pod",
  "apiVersion": "v1",
  "metadata": {
    "name": "hello-7747bc55f-2t4tb",
    "generateName": "hello-7747bc55f-",
    "namespace": "default",
    "selfLink": "/api/v1/namespaces/default/pods/hello-7747bc55f-2t4tb",
    "uid": "234edbc5-5c11-4622-8773-b9ebf36e544d",
    "resourceVersion": "960",
    "creationTimestamp": "2019-12-31T14:28:15Z",
    "labels": {
      "app": "hello",
      "pod-template-hash": "7747bc55f"
    },
    "ownerReferences": [
      {
        "apiVersion": "apps/v1",
        "kind": "ReplicaSet",
        "name": "hello-7747bc55f",
        "uid": "94023a2f-3d0d-4eda-b69f-c2b4b6071cc2",
        "controller": true,
        "blockOwnerDeletion": true
      }
    ]
  },
  "spec": {
    "volumes": [
      {
        "name": "default-token-ltwvj",
        "secret": {
          "secretName": "default-token-ltwvj",
          "defaultMode": 420
        }
      }
    ],
    "containers": [
      {
        "name": "node-hello",
        "image": "gcr.io/google-samples/node-hello:1.0",
        "resources": {
          
        },
        "volumeMounts": [
          {
            "name": "default-token-ltwvj",
            "readOnly": true,
            "mountPath": "/var/run/secrets/kubernetes.io/serviceaccount"
          }
        ],
        "terminationMessagePath": "/dev/termination-log",
        "terminationMessagePolicy": "File",
        "imagePullPolicy": "IfNotPresent"
      }
    ],
    "restartPolicy": "Always",
    "terminationGracePeriodSeconds": 30,
    "dnsPolicy": "ClusterFirst",
    "serviceAccountName": "default",
    "serviceAccount": "default",
    "nodeName": "newyear-worker2",
    "securityContext": {
      
    },
    "schedulerName": "default-scheduler",
    "tolerations": [
      {
        "key": "node.kubernetes.io/not-ready",
        "operator": "Exists",
        "effect": "NoExecute",
        "tolerationSeconds": 300
      },
      {
        "key": "node.kubernetes.io/unreachable",
        "operator": "Exists",
        "effect": "NoExecute",
        "tolerationSeconds": 300
      }
    ],
    "priority": 0,
    "enableServiceLinks": true
  },
  "status": {
    "phase": "Running",
    "conditions": [
      {
        "type": "Initialized",
        "status": "True",
        "lastProbeTime": null,
        "lastTransitionTime": "2019-12-31T14:28:15Z"
      },
      {
        "type": "Ready",
        "status": "True",
        "lastProbeTime": null,
        "lastTransitionTime": "2019-12-31T14:28:39Z"
      },
      {
        "type": "ContainersReady",
        "status": "True",
        "lastProbeTime": null,
        "lastTransitionTime": "2019-12-31T14:28:39Z"
      },
      {
        "type": "PodScheduled",
        "status": "True",
        "lastProbeTime": null,
        "lastTransitionTime": "2019-12-31T14:28:15Z"
      }
    ],
    "hostIP": "172.17.0.2",
    "podIP": "10.244.1.2",
    "podIPs": [
      {
        "ip": "10.244.1.2"
      }
    ],
    "startTime": "2019-12-31T14:28:15Z",
    "containerStatuses": [
      {
        "name": "node-hello",
        "state": {
          "running": {
            "startedAt": "2019-12-31T14:28:38Z"
          }
        },
        "lastState": {
          
        },
        "ready": true,
        "restartCount": 0,
        "image": "gcr.io/google-samples/node-hello:1.0",
        "imageID": "sha256:9ef4b4c241fc28ce894a57a7cb218b796838fdccf15515d784791abc2ffc1ee6",
        "containerID": "containerd://61562b770e3723a442af35e32d15998c01de8da7fa6eeaf49058dcc9c31b5f63",
        "started": true
      }
    ],
    "qosClass": "BestEffort"
  }
}

Pretty verbose, huh...
At least, it enables to demonstrate that we can access the cluster from the 
host machine, and poll directly the REST APIs to retrieve information about 
the currently rdeployed applications. Interestingly, doing so manually, we 
operate exactly the same way kubectl or the dashboard do: they poll the REST 
APIs exposed by the cluster.



The url structure is self explicit:
    - it specifies the API version (v1 since versions 1.15 at least)
    - it then indicated the namespace (here: 'default')
    - and then indicates the resource we need to poll (pods, deployment...).
      In our case, we indicate 'pods' and which Pod (with its name) we want 
      information on.


4.5 - View the container logs
==============================

Anything that the application would normally send to STDOUT becomes logs for 
the container within the Pod. We can retrieve these logs using the kubectl 
logs command:

tso@laptop:~$kubectl logs $POD_NAME
(no answer!!!)

We still face the same issue that the cluster surprisingly knows the Pod, but 
cannot retrieve the logs from it. Maybe is it related to the way I have 
configured the cluster...

Note: We don’t need to specify the container name, because we only have one 
container inside the pod.


4.6 - Executing command inside the container
============================================

We can execute commands directly in the container once the Pod is up and 
running. For this, we use the exec command and use the name of the Pod as a 
parameter. Let’s list the environment variables:

tso@laptop:~$ kubectl exec $POD_NAME env
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
HOSTNAME=hello-7747bc55f-2t4tb
NPM_CONFIG_LOGLEVEL=info
NODE_VERSION=4.4.2
KUBERNETES_PORT=tcp://10.96.0.1:443
KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443
KUBERNETES_PORT_443_TCP_PROTO=tcp
KUBERNETES_PORT_443_TCP_PORT=443
KUBERNETES_PORT_443_TCP_ADDR=10.96.0.1
KUBERNETES_SERVICE_HOST=10.96.0.1
KUBERNETES_SERVICE_PORT=443
KUBERNETES_SERVICE_PORT_HTTPS=443
HOME=/root

Again, worth mentioning that the name of the container itself can be omitted 
since we only have a single container in the Pod.

Next let’s start a bash session in the Pod’s container:

tso@laptop:~$ kubectl exec -ti $POD_NAME bash
root@hello-7747bc55f-2t4tb:/# 

We have now an open console on the container where we run our NodeJS 
application, and we are logged as root. The source code of the app is in the 
server.js file:

root@hello-7747bc55f-2t4tb:/# cat server.js
// Copyright 2016, Google, Inc.
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//    http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

'use strict';

// [START all]
var http = require('http');
var handleRequest = function(request, response) {
    response.writeHead(200);
    response.end('Hello Kubernetes!');
};
var www = http.createServer(handleRequest);
www.listen(process.env.PORT || 8080);
// [END all]


You can check that the application is up by running a curl command:

root@hello-7747bc55f-2t4tb:/# curl localhost:8080
Hello Kubernetes!

    Note: here we used localhost because we executed the command inside the 
        NodeJS Pod. If you cannot connect to localhost:8080, check to make 
        sure you have run the kubectl exec command and are launching the 
        command from within the Pod

To close your container connection type "exit".



3.5 - Expose Your App Publicly
==============================


3.5.1 - Overview of Kubernetes Services
=======================================

Kubernetes Pods are mortal. Pods in fact have a lifecycle. When a worker node 
dies, the Pods running on the Node are also lost. A ReplicaSet might then 
dynamically drive the cluster back to desired state via creation of new Pods 
to keep your application running. As another example, consider an 
image-processing backend with 3 replicas. Those replicas are exchangeable; the 
front-end system should not care about backend replicas or even if a Pod is 
lost and recreated. That said, each Pod in a Kubernetes cluster has a unique 
IP address, even Pods on the same Node, so there needs to be a way of 
automatically reconciling changes among Pods so that your applications 
continue to function.

A Service in Kubernetes is an abstraction which defines a logical set of Pods 
and a policy by which to access them. Services enable a loose coupling between 
dependent Pods. A Service is defined using YAML (preferred) or JSON, like all 
Kubernetes objects. The set of Pods targeted by a Service is usually 
determined by a LabelSelector: a LabelSelector is a the usual way Kubernetes 
will identify the right pods.

Although each Pod has a unique IP address, those IPs are not exposed outside 
the cluster without a Service. Services allow your applications to receive 
traffic (from outside or from other applications runnig on the same cluster). 
Services can be exposed in different ways by specifying a type in the 
ServiceSpec:

    - ClusterIP (default) - Exposes the Service on an internal IP in the 
        cluster. This type makes the Service only reachable from within the 
        cluster, by other services running on the same cluster.
    - NodePort - Exposes the Service on the same port of each selected Node in 
        the cluster using NAT. Makes a Service accessible from outside the 
        cluster using <NodeIP>:<NodePort>. Superset of ClusterIP.
        The principle here is that the <NodeIP> is shared by all services 
        running on the same cluster: they only differentiate one from another 
        by their port number. This is very convenient to enable a controlled 
        communication between services, typically when they should primarily 
        get exposed via an API gateway which will handle security and access 
        rights.
    - LoadBalancer - Creates an external load balancer in the current cloud 
        (if supported) and assigns a fixed, external IP to the Service. 
        Superset of NodePort.
        Here, the service has its own IP and its own (set of) port(s): it is 
        very easily reacheable from outside the cluster, which implies that it 
        must embed security by design.
    - ExternalName - Exposes the Service using an arbitrary name (specified by 
        externalName in the spec) by returning a CNAME record with the name. 
        No proxy is used (requires v1.7 or higher of kube-dns).


Additionally, note that there are some use cases with Services that involve 
not defining selector in the spec. A Service created without selector will 
also not create the corresponding Endpoints object. This allows users to 
manually map a Service to specific endpoints. Another possibility why there 
may be no selector is you are strictly using type: ExternalName.


3.5.2 - Services and Labels
===========================

(image: "expose your app - 1")

A Service routes traffic across a set of Pods. Services are the abstraction 
that allow pods to die and replicate in Kubernetes without impacting your 
application. Discovery and routing among dependent Pods (such as the frontend 
and backend components in an application) is handled by Kubernetes Services.

Services match a set of Pods using labels and selectors, a grouping primitive 
that allows logical operation on objects in Kubernetes. Labels are key/value 
pairs attached to objects and can be used in any number of ways:

    - Designate objects for development, test, and production
    - Embed version tags
    - Classify an object using tags


(image: "expose your app - 2")

Labels can be attached to objects at creation time or later on. They can be 
modified at any time. Let's expose our application now using a Service and 
apply some labels.


3.5.3 - Create a new service
============================


Let’s verify that our application is running. We’ll use the kubectl get 
command and look for existing Pods:

tso@laptop:~$ kubectl get pods
NAME                    READY   STATUS    RESTARTS   AGE
hello-7747bc55f-2t4tb   1/1     Running   0          19m

Next, let’s list the current Services from our cluster:

tso@laptop:~$ kubectl get services
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   24m

We have a Service called kubernetes that is created by default when the cluster  
starts. To create a new service and expose it to external traffic we’ll use the 
'expose' command with NodePort as parameter.

tso@laptop:~$ kubectl expose deployment/hello --type="NodePort" --port 8080
service/hello exposed

Let’s run again the get services command:

tso@laptop:~$ kubectl get services
NAME         TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)          AGE
hello        NodePort    10.96.14.33   <none>        8080:32267/TCP   20s
kubernetes   ClusterIP   10.96.0.1     <none>        443/TCP          25m

Yes: we have now a running Service called 'hello'. Here we see that the Service 
received a unique cluster-IP, an internal port and an external-IP (the IP of 
the Node).

To find out what port was opened externally (by the NodePort option) we’ll run 
the describe service command:

First we need to identify the port which is exposed by the NodePort option:

tso@laptop:~$ kubectl describe services/hello
Name:                     hello
Namespace:                default
Labels:                   app=hello
Annotations:              <none>
Selector:                 app=hello
Type:                     NodePort
IP:                       10.96.14.33
Port:                     <unset>  8080/TCP
TargetPort:               8080/TCP
NodePort:                 <unset>  32267/TCP
Endpoints:                10.244.1.2:8080
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   <none>

We ceate an environment variable called NODE_PORT that has the value of the 
Node port assigned:

tso@laptop:~$ export NODE_PORT=32267

Then we need to identify the external IP which is exposed for the whole 
cluster: this ip is used as teh 'endpoint' for the default 'kubernetes' 
service:

tso@laptop:~$ kubectl describe services/kubernetes
Name:              kubernetes
Namespace:         default
Labels:            component=apiserver
                   provider=kubernetes
Annotations:       <none>
Selector:          <none>
Type:              ClusterIP
IP:                10.96.0.1
Port:              https  443/TCP
TargetPort:        6443/TCP
Endpoints:         172.17.0.4:6443
Session Affinity:  None
Events:            <none>

Here we can see the cluster's shared Endpoint: 172.17.0.4.

tso@laptop:~$ export ENDPOINT=172.17.0.4

Now that we have both the ip@ (172.17.0.4) and the port (32267), we can test 
that the app is exposed outside of the cluster using curl:

tso@laptop:~$ curl $ENDPOINT:$NODE_PORT
Hello Kubernetes!
tso@laptop:~$ 

And we get a response from the server. The Service is exposed.


3.5.4 - Using labels
====================

The Deployment created automatically a label for our Pod. With describe 
deployment command you can see the name of the label:

$ kubectl describe deployment

Let’s use this label to query our list of Pods. We’ll use the kubectl get pods 
command with -l as a parameter, followed by the label values:

$ kubectl get pods -l run=kubernetes-bootcamp

You can do the same to list the existing services:

$ kubectl get services -l run=kubernetes-bootcamp

Get the name of the Pod and store it in the POD_NAME environment variable:

$ export POD_NAME=$(kubectl get pods -o go-template --template '{{range .items}}{{.metadata.name}}{{"\n"}}{{end}}')
echo Name of the Pod: $POD_NAME

To apply a new label we use the label command followed by the object type, 
object name and the new label:

$ kubectl label pod $POD_NAME app=v1

This will apply a new label to our Pod (we pinned the application version to 
the Pod), and we can check it with the describe pod command:

$ kubectl describe pods $POD_NAME

We see here that the label is attached now to our Pod. And we can query now 
the list of pods using the new label:

$ kubectl get pods -l app=v1

And we see the Pod.


3.5.5 - Deleting a service
=========================

To delete Services you can use the delete service command. Labels can be used 
also here:

$ kubectl delete service -l run=kubernetes-bootcamp

Confirm that the service is gone:

$ kubectl get services

This confirms that our Service was removed. To confirm that route is not 
exposed anymore you can curl the previously exposed IP and port:

$ curl $(minikube ip):$NODE_PORT

This proves that the app is not reachable anymore from outside of the cluster. 
You can confirm that the app is still running with a curl inside the pod:

$ kubectl exec -ti $POD_NAME curl localhost:8080

We see here that the application is up. This is because the Deployment is 
managing the application. To shut down the application, you would need to 
delete the Deployment as well.



3.6 - Scale Your App
====================


3.6.1 - Explanation - Scaling an application
============================

In the previous modules we created a Deployment, and then exposed it publicly 
via a Service. The Deployment created only one Pod for running our application. 
When traffic increases, we will need to scale the application to keep up with 
user demand.

Scaling is accomplished by changing the number of replicas in a Deployment.

(image - "scale your app - 1")
(image - "scale your app - 2")

Scaling out a Deployment will ensure new Pods are created and scheduled to 
Nodes with available resources. Scaling will increase the number of Pods to the 
new desired state. Kubernetes also supports autoscaling of Pods, but it is 
outside of the scope of this tutorial. Scaling to zero is also possible, and it 
will terminate all Pods of the specified Deployment.

Running multiple instances of an application will require a way to distribute 
the traffic to all of them. Services have an integrated load-balancer that will 
distribute network traffic to all Pods of an exposed Deployment. Services will 
monitor continuously the running Pods using endpoints, to ensure the traffic is 
sent only to available Pods.


Once you have multiple instances of an Application running, you would be able 
to do Rolling updates without downtime. We'll cover that in the next module. 
Now, let's go to the online terminal and scale our application.


3.6.2 - Scaling a deployment
============================

To list your deployments use the get deployments command:

$ kubectl get deployments

We should have 1 Pod. If not, run the command again. This shows:

    - READY shows the ratio of CURRENT to DESIRED replicas
        * CURRENT is the number of replicas running now
        * DESIRED is the configured number of replicas
    - UP-TO-DATE is the number of replicas that were updated to match the 
        desired (configured) state
    - AVAILABLE state shows how many replicas are actually AVAILABLE to the 
        users

Next, let’s scale the Deployment to 4 replicas. We’ll use the kubectl scale 
command, followed by the deployment type, name and desired number of 
instances:

$ kubectl scale deployments/kubernetes-bootcamp --replicas=4

To list your Deployments once again, use get deployments:

$ kubectl get deployments

The change was applied, and we have 4 instances of the application available. 
Next, let’s check if the number of Pods changed:

$ kubectl get pods -o wide

There are 4 Pods now, with different IP addresses. The change was registered 
in the Deployment events log. To check that, use the describe command:

$ kubectl describe deployments/kubernetes-bootcamp

You can also view in the output of this command that there are 4 replicas now.


3.6.3 - Load Balancing
======================

Let’s check that the Service is load-balancing the traffic. To find out the 
exposed IP and Port we can use the describe service as we learned in the 
previously Module:

$ kubectl describe services/kubernetes-bootcamp

Create an environment variable called NODE_PORT that has a value as the Node 
port:

$ export NODE_PORT=$(kubectl get services/kubernetes-bootcamp -o go-template='{{(index .spec.ports 0).nodePort}}')
echo NODE_PORT=$NODE_PORT

Next, we’ll do a curl to the exposed IP and port. Execute the command multiple 
times:

$ curl $(minikube ip):$NODE_PORT

We hit a different Pod with every request. This demonstrates that the 
load-balancing is working.


3.6.4 - Scale Down
==================

To scale down the Service to 2 replicas, run again the scale command:

$ kubectl scale deployments/kubernetes-bootcamp --replicas=2

List the Deployments to check if the change was applied with the get 
deployments command:

$ kubectl get deployments

The number of replicas decreased to 2. List the number of Pods, with get pods:

$ kubectl get pods -o wide

This confirms that 2 Pods were terminated.



3.7 - Update Your App
=====================


3.7.1 - Updating an application
=============================

Users expect applications to be available all the time and developers are 
expected to deploy new versions of them several times a day. In Kubernetes 
this is done with rolling updates. Rolling updates allow Deployments' update 
to take place with zero downtime by incrementally updating Pods instances with 
new ones. The new Pods will be scheduled on Nodes with available resources.

In the previous module we scaled our application to run multiple instances. 
This is a requirement for performing updates without affecting application 
availability. By default, the maximum number of Pods that can be unavailable 
during the update and the maximum number of new Pods that can be created, is 
one. Both options can be configured to either numbers or percentages (of 
Pods). In Kubernetes, updates are versioned and any Deployment update can be 
reverted to previous (stable) version.


3.7.2 - Rolling updates overview
==============================

(image: "update your app - 1")
(image: "update your app - 2")
(image: "update your app - 3")
(image: "update your app - 4")

Similar to application Scaling, if a Deployment is exposed publicly, the 
Service will load-balance the traffic only to available Pods during the 
update. An available Pod is an instance that is available to the users of the 
application.

Rolling updates allow the following actions:

    - Promote an application from one environment to another (via container 
        image updates)
    - Rollback to previous versions
    - Continuous Integration and Continuous Delivery of applications with zero 
        downtime


3.7.3 - Update the version of the app
=====================================

To list your deployments use the get deployments command: 

$ kubectl get deployments

To list the running Pods use the get pods command:

$ kubectl get pods

To view the current image version of the app, run a describe command against 
the Pods (look at the Image field):

$ kubectl describe pods

To update the image of the application to version 2, use the set image 
command, followed by the deployment name and the new image version:

$ kubectl set image deployments/kubernetes-bootcamp kubernetes-bootcamp=jocatalin/kubernetes-bootcamp:v2

The command notified the Deployment to use a different image for your app and 
initiated a rolling update. Check the status of the new Pods, and view the old 
one terminating with the get pods command:

$ kubectl get pods


3.7.4 - Verify an update
========================

First, let’s check that the App is running. To find out the exposed IP and 
Port we can use describe service:

$ kubectl describe services/kubernetes-bootcamp

Create an environment variable called NODE_PORT that has the value of the Node 
port assigned:

$ export NODE_PORT=$(kubectl get services/kubernetes-bootcamp -o go-template='{{(index .spec.ports 0).nodePort}}')
echo NODE_PORT=$NODE_PORT

Next, we’ll do a curl to the the exposed IP and port:

$ curl $(minikube ip):$NODE_PORT

We hit a different Pod with every request and we see that all Pods are running 
the latest version (v2).

The update can be confirmed also by running a rollout status command:

$ kubectl rollout status deployments/kubernetes-bootcamp

To view the current image version of the app, run a describe command against 
the Pods:

$ kubectl describe pods

We run now version 2 of the app (look at the Image field)


3.7.5 - Rollback an update
==========================

Let’s perform another update, and deploy image tagged as v10 :

$ kubectl set image deployments/kubernetes-bootcamp kubernetes-bootcamp=gcr.io/google-samples/kubernetes-bootcamp:v10

Use get deployments to see the status of the deployment:

$ kubectl get deployments

And something is wrong… We do not have the desired number of Pods available. 
List the Pods again:

$ kubectl get pods

A describe command on the Pods should give more insights:

$ kubectl describe pods

There is no image called v10 in the repository. Let’s roll back to our 
previously working version. We’ll use the rollout undo command:

$ kubectl rollout undo deployments/kubernetes-bootcamp

The rollout command reverted the deployment to the previous known state 
(v2 of the image). Updates are versioned and you can revert to any previously 
know state of a Deployment. List again the Pods:

$ kubectl get pods

Four Pods are running. Check again the image deployed on the them:

$ kubectl describe pods

We see that the deployment is using a stable version of the app (v2). The 
Rollback was successful.


